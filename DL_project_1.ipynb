{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNhJo7o6797gxysHMZ5h3Bp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sameedsyed1/website/blob/main/DL_project_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Deep Learning and Neural Networks\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Z72TCX2EvwFE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Name: Syed Muhammad Sameed Hussain\n",
        "\n",
        "I.D: F2019332008\n",
        "\n",
        "instructor: Dr. Saira Osama\n",
        "\n",
        "section: D1\n"
      ],
      "metadata": {
        "id": "PRWwMNKgu_yD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we need to define the structure of our Neural Network. Because our dataset is relatively simple, a network with just a hidden layer will do fine. So we will have an input layer, a hidden layer and an output layer. Next, we need an activation function. The sigmoid function is a good choice for the last layer because it outputs values between 0 and 1 while tanh (hyperbolic tangent) works better in the hidden layer."
      ],
      "metadata": {
        "id": "OprQ5eo7uqLf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, the parameters to be learned are the weights W1, W2 and the biases b1, b2. As you can see W1 and b1 connect the input layer with the hidden layer while W2, b2 connect the hidden layer with the output layer. From the basic theory we know that the activations A1 and A2 are calculated as follows:\n",
        "\n",
        "A1 = h(W1*X + b1)\n",
        "A2 = g(W2*A1 + b2)"
      ],
      "metadata": {
        "id": "CG3bPyL-w4UC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Where g and h are the two activation functions we chose (sigmoid and tanh) and W1, W1, b1, b2 are generally Matrices."
      ],
      "metadata": {
        "id": "HtXhXwbtxFev"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First we will implement our sigmoid activation function defined as follows: g(z) = 1/(1+e^(-z)) where z will be a matrix in general."
      ],
      "metadata": {
        "id": "BnzSmC4SxGfp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def sigmoid(z):\n",
        "  return 1/(1 + np.exp(-z))"
      ],
      "metadata": {
        "id": "3AdDCdXJxvjn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we have to initialize our parameters. Weight matrices W1 and W2 will be randomly initialized from a normal distribution while biases b1 and b2 will be initialized to zero. The function initialize_parameters(n_x, n_h, n_y) takes as input the number of units in each of the 3 layers and initializes the parameters properly."
      ],
      "metadata": {
        "id": "Al9O2vMEx5pM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_parameters(n_x, n_h, n_y):\n",
        "  W1 = np.random.randn(n_h, n_x)\n",
        "  b1 = np.zeros((n_h, 1))\n",
        "  W2 = np.random.randn(n_y, n_h)\n",
        "  b2 = np.zeros((n_y, 1))\n",
        "\n",
        "  parameters = {\n",
        "    \"W1\": W1,\n",
        "    \"b1\" : b1,\n",
        "    \"W2\": W2,\n",
        "    \"b2\" : b2\n",
        "  }\n",
        "  return parameters"
      ],
      "metadata": {
        "id": "bVjn1lANyATm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next step is to implement the Forward Propagation. "
      ],
      "metadata": {
        "id": "7l4L1zd7yRxK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def forward_prop(X, parameters):\n",
        "  W1 = parameters[\"W1\"]\n",
        "  b1 = parameters[\"b1\"]\n",
        "  W2 = parameters[\"W2\"]\n",
        "  b2 = parameters[\"b2\"]\n",
        "\n",
        "  Z1 = np.dot(W1, X) + b1\n",
        "  A1 = np.tanh(Z1)\n",
        "  Z2 = np.dot(W2, A1) + b2\n",
        "  A2 = sigmoid(Z2)\n",
        "\n",
        "  cache = {\n",
        "    \"A1\": A1,\n",
        "    \"A2\": A2\n",
        "  }\n",
        "  return A2, cache"
      ],
      "metadata": {
        "id": "ri76vqUqyS5s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now have to compute the loss function."
      ],
      "metadata": {
        "id": "f2g9G3F_ycgg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_cost(A2, Y):\n",
        "  cost = -np.sum(np.multiply(Y, np.log(A2)) +  np.multiply(1-Y, np.log(1-A2)))/m\n",
        "  cost = np.squeeze(cost)\n",
        "\n",
        "  return cost"
      ],
      "metadata": {
        "id": "ZtuEPGgIymtC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we calculate the Back Propagation. This function will return the gradients of the Loss function with respect to the 4 parameters of our network(W1, W2, b1, b2):"
      ],
      "metadata": {
        "id": "gxR0ZtnnyvQF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def backward_prop(X, Y, cache, parameters):\n",
        "  A1 = cache[\"A1\"]\n",
        "  A2 = cache[\"A2\"]\n",
        "\n",
        "  W2 = parameters[\"W2\"]\n",
        "\n",
        "  dZ2 = A2 - Y\n",
        "  dW2 = np.dot(dZ2, A1.T)/m\n",
        "  db2 = np.sum(dZ2, axis=1, keepdims=True)/m\n",
        "  dZ1 = np.multiply(np.dot(W2.T, dZ2), 1-np.power(A1, 2))\n",
        "  dW1 = np.dot(dZ1, X.T)/m\n",
        "  db1 = np.sum(dZ1, axis=1, keepdims=True)/m\n",
        "\n",
        "  grads = {\n",
        "    \"dW1\": dW1,\n",
        "    \"db1\": db1,\n",
        "    \"dW2\": dW2,\n",
        "    \"db2\": db2\n",
        "  }\n",
        "\n",
        "  return grads"
      ],
      "metadata": {
        "id": "kOhv1yx7y_vY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we have all the gradients of the Loss function, so we can proceed to the actual learning! We will use Gradient Descent algorithm to update our parameters and make our model learn with the learning rate passed as a parameter:"
      ],
      "metadata": {
        "id": "g_sXGVrnzIho"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def update_parameters(parameters, grads, learning_rate):\n",
        "  W1 = parameters[\"W1\"]\n",
        "  b1 = parameters[\"b1\"]\n",
        "  W2 = parameters[\"W2\"]\n",
        "  b2 = parameters[\"b2\"]\n",
        "\n",
        "  dW1 = grads[\"dW1\"]\n",
        "  db1 = grads[\"db1\"]\n",
        "  dW2 = grads[\"dW2\"]\n",
        "  db2 = grads[\"db2\"]\n",
        "\n",
        "  W1 = W1 - learning_rate*dW1\n",
        "  b1 = b1 - learning_rate*db1\n",
        "  W2 = W2 - learning_rate*dW2\n",
        "  b2 = b2 - learning_rate*db2\n",
        "  \n",
        "  new_parameters = {\n",
        "    \"W1\": W1,\n",
        "    \"W2\": W2,\n",
        "    \"b1\" : b1,\n",
        "    \"b2\" : b2\n",
        "  }\n",
        "\n",
        "  return new_parameters"
      ],
      "metadata": {
        "id": "ZQvGCXuIzQIZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we just put them all together inside a function called model() and call model() from the main program.\n",
        "\n",
        "Model() function takes as input the features matrix X, the labels matrix Y, the number of units n_x, n_h, n_y, the number of iterations we want our Gradient Descent algorithm to run and the learning rate of Gradient Descent and combines all the functions above to return the trained parameters of our model:"
      ],
      "metadata": {
        "id": "7jjZh6wkzuV7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def model(X, Y, n_x, n_h, n_y, num_of_iters, learning_rate):\n",
        "  parameters = initialize_parameters(n_x, n_h, n_y)\n",
        "\n",
        "  for i in range(0, num_of_iters+1):\n",
        "    a2, cache = forward_prop(X, parameters)\n",
        "\n",
        "    cost = calculate_cost(a2, Y)\n",
        "\n",
        "    grads = backward_prop(X, Y, cache, parameters)\n",
        "\n",
        "    parameters = update_parameters(parameters, grads, learning_rate)\n",
        "\n",
        "    if(i%100 == 0):\n",
        "      print('Cost after iteration# {:d}: {:f}'.format(i, cost))\n",
        "\n",
        "  return parameters"
      ],
      "metadata": {
        "id": "iVf5n4AWz2JU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we just have to make our prediction. The function predict(X, parameters) takes as input the matrix X with elements the 2 numbers for which we want to compute the XOR function and the trained parameters of the model and returns the desired result y_predict by using a threshold of 0.5:"
      ],
      "metadata": {
        "id": "4akoj52O0AsK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(X, parameters):\n",
        "  a2, cache = forward_prop(X, parameters)\n",
        "  yhat = a2\n",
        "  yhat = np.squeeze(yhat)\n",
        "  if(yhat >= 0.5):\n",
        "    y_predict = 1\n",
        "  else:\n",
        "    y_predict = 0\n",
        "\n",
        "  return y_predict"
      ],
      "metadata": {
        "id": "aECgKK-20S-T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Now let’s go to the main program and declare our matrices X, Y and the hyperparameters n_x, n_h, n_y, num_of_iters, learning_rate:"
      ],
      "metadata": {
        "id": "dx0iG2o10IW-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(2)\n",
        "\n",
        "# The 4 training examples by columns\n",
        "X = np.array([[0, 0, 1, 1], [0, 1, 0, 1]])\n",
        "\n",
        "# The outputs of the XOR for every example in X\n",
        "Y = np.array([[0, 1, 1, 0]])\n",
        "\n",
        "# No. of training examples\n",
        "m = X.shape[1]\n",
        "# Set the hyperparameters\n",
        "n_x = 2     #No. of neurons in first layer\n",
        "n_h = 2     #No. of neurons in hidden layer\n",
        "n_y = 1     #No. of neurons in output layer\n",
        "num_of_iters = 1000\n",
        "learning_rate = 0.3"
      ],
      "metadata": {
        "id": "SIh7Chba0pv_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trained_parameters = model(X, Y, n_x, n_h, n_y, num_of_iters, learning_rate)"
      ],
      "metadata": {
        "id": "F_LkpRnI0_Ep"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We make our prediction for a random pair of numbers, e.g (1,1):"
      ],
      "metadata": {
        "id": "cYO6IAAu1DPN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test 2X1 vector to calculate the XOR of its elements. \n",
        "# You can try any of those: (0, 0), (0, 1), (1, 0), (1, 1)\n",
        "X_test = np.array([[1], [1]])\n",
        "y_predict = predict(X_test, trained_parameters)\n",
        "# Print the result\n",
        "print('Neural Network prediction for example ({:d}, {:d}) is {:d}'.format(\n",
        "    X_test[0][0], X_test[1][0], y_predict))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jMTSQKRA1KH7",
        "outputId": "593b3a84-f110-4649-918b-997861136d21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Neural Network prediction for example (1, 1) is 0\n"
          ]
        }
      ]
    }
  ]
}